{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Examples of Use and Graphs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "import torch\n",
    "import copy\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## If You're Just Looking for the Graphs!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These can be found in the notebooks:\n",
    "\n",
    "- `graph_synthetic.ipynb`\n",
    "- `graph_ecg.ipynb`\n",
    "- `graph_cifarn_different_noise.ipynb`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the Optimisers:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The key part of this research is the optimisers, modified to perform Loss Adapted Plasticity (LAP) training. These have been implemented in Pytorch by modifying the Adam and SGD classes. These modified versions can be imported using:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from loss_adapted_plasticity import LAP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Documentation for both has been updated to reflect the modifications:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class LAP in module loss_adapted_plasticity.lap_wrapper:\n",
      "\n",
      "class LAP(builtins.object)\n",
      " |  LAP(optimizer: torch.optim.optimizer.Optimizer, lap_n: int = 10, depression_strength: float = 1.0, depression_function='discrete_ranking_std', depression_function_kwargs: dict = {}, source_is_bool: bool = False, **opt_kwargs)\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __getattr__(self, name)\n",
      " |  \n",
      " |  __getstate__(self)\n",
      " |      # defined since __getattr__ causes pickling problems\n",
      " |  \n",
      " |  __init__(self, optimizer: torch.optim.optimizer.Optimizer, lap_n: int = 10, depression_strength: float = 1.0, depression_function='discrete_ranking_std', depression_function_kwargs: dict = {}, source_is_bool: bool = False, **opt_kwargs)\n",
      " |      Depression won't be applied until at least :code:`lap_n` loss values\n",
      " |      have been collected for at least two sources. This could be \n",
      " |      longer if a :code:`hold_off` parameter is used in the depression function.\n",
      " |      \n",
      " |      This class will wrap any optimiser and perform lap gradient depression\n",
      " |      before the values are passed to the underlying optimiser.\n",
      " |      \n",
      " |      \n",
      " |      Examples\n",
      " |      ---------\n",
      " |      \n",
      " |      The following wraps the Adam optimiser with the LAP functionality.\n",
      " |      \n",
      " |      .. code-block::\n",
      " |          \n",
      " |          >>> optimizer = LAP(\n",
      " |          ...     torch.optim.Adam, params=model.parameters(), lr=0.01,\n",
      " |          ...     )\n",
      " |      \n",
      " |      Ensure that when using this optimiser, during the :code:`.step`\n",
      " |      method, you use the arguments :code:`loss` and :code:`source`. \n",
      " |      For example::\n",
      " |      \n",
      " |          >>> loss = loss.backward()\n",
      " |          >>> optimizer.step(loss, source)\n",
      " |      \n",
      " |      \n",
      " |      Arguments\n",
      " |      ---------\n",
      " |      \n",
      " |      - optimizer: torch.optim.Optimizer:\n",
      " |          The optimizer to wrap with the LAP algorithm.\n",
      " |      \n",
      " |      - lap_n: int, optional:\n",
      " |          The number of previous loss values for each source\n",
      " |          to be used in the loss adapted plasticity\n",
      " |          calculations.\n",
      " |          Defaults to :code:`10`.\n",
      " |      \n",
      " |      - depression_strength: float:\n",
      " |          This float determines the strength of the depression\n",
      " |          applied to the gradients. It is the value of m in \n",
      " |          dep = 1-tanh(m*d)**2.\n",
      " |          Defaults to :code:`1`.\n",
      " |      \n",
      " |      - depression_function: function or string, optional:\n",
      " |          This is the function used to calculate the depression\n",
      " |          based on the loss array (with sources containing full \n",
      " |          loss history) and the source of the current batch. \n",
      " |          Ensure that the first two arguments of this function are\n",
      " |          loss_array and source_idx.\n",
      " |          If string, please ensure it is 'discrete_ranking_std'\n",
      " |          Defaults to :code:`'discrete_ranking_std'`.\n",
      " |      \n",
      " |      - depression_function_kwargs: dict, optional:\n",
      " |          Keyword arguments that will be used in depression_function\n",
      " |          when initiating it, if it is specified by a string.\n",
      " |          Defaults to :code:`{}`.\n",
      " |      \n",
      " |      - source_is_bool: bool, optional:\n",
      " |          This tells the optimizer that the sources will be named True\n",
      " |          when the source is corrupted and False if the source is not.\n",
      " |          If the incoming source is corrupted, then the optimizer will not\n",
      " |          make a step.\n",
      " |          Defaults to :code:`False`.\n",
      " |  \n",
      " |  __setstate__(self, state)\n",
      " |      # defined since __getattr__ causes pickling problems\n",
      " |  \n",
      " |  step(self, loss: float, source: Hashable, override_dep: Union[bool, NoneType] = None, writer: Union[torch.utils.tensorboard.writer.SummaryWriter, NoneType] = None, **kwargs)\n",
      " |      Performs a single optimization step.\n",
      " |      \n",
      " |      Arguments\n",
      " |      ---------\n",
      " |      \n",
      " |      - loss: float:\n",
      " |          This is the loss value that is used in the depression calculations.\n",
      " |      \n",
      " |      - source: hashable:\n",
      " |          This is the source name that is used to\n",
      " |          store the loss values for the different sources.\n",
      " |      \n",
      " |      - override_dep: bool or None:\n",
      " |          If None, then whether to apply depression will be decided\n",
      " |          based on the logic of this class. If True, then depression will \n",
      " |          be applied. This might cause unexpected results if there is no depression value\n",
      " |          calculated based on whether there is enough data available in the \n",
      " |          .loss_array. In this case, not depression is applied.\n",
      " |          If False, then depression will not be applied.\n",
      " |          This is mostly useful as an option to turn off LAP.\n",
      " |          Defaults to :code:`None`.\n",
      " |      \n",
      " |      - writer: torch.utils.tensorboard.SummaryWriter:\n",
      " |          A tensorboard writer can be passed into this function to track metrics.\n",
      " |          Defaults to :code:`None`.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors defined here:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(LAP)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These optimisers can be used in exactly the same way that you have previously used other pytorch optimisers, except for the fact that they take two extra arguments in the step method.\n",
    "\n",
    "In current code, the optimisation process looks as follows:\n",
    "\n",
    "```python\n",
    "inputs, labels = data_batch\n",
    "# ======= forward ======= \n",
    "outputs = model(inputs)\n",
    "loss = criterion(outputs, labels)\n",
    "# ======= backward =======\n",
    "loss.backward()\n",
    "optimizer.step()\n",
    "```\n",
    "\n",
    "When using a LAP optimiser, this needs to be changed to (with the key difference being the optimiser step ```optimizer.step(loss, source)```):\n",
    "\n",
    "```python\n",
    "inputs, labels = data_batch\n",
    "# ======= forward ======= \n",
    "outputs = model(inputs)\n",
    "loss = criterion(outputs, labels)\n",
    "# ======= backward =======\n",
    "loss.backward()\n",
    "optimizer.step(loss, source)\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To wrap any optimiser, use:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_model = torch.nn.Linear(100,10)\n",
    "\n",
    "optimizer = LAP(\n",
    "    torch.optim.Adam, \n",
    "    params=example_model.parameters(), \n",
    "    lr=0.01,\n",
    "    lap_n=25,\n",
    "    depression_strength=1.0,\n",
    "    # below is passed as dict as different depression functions can easily be addded\n",
    "    depression_function_kwargs={'strictness': 0.8} \n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ready to Fit Models:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Two models have already been set up with the use of these optimisers built in. To access these, please use the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from experiment_code.models.model_code.mlp import MLPLearning\n",
    "from experiment_code.models.model_code.conv3net import Conv3NetLearning\n",
    "from experiment_code.models.model_code.resnet import ResNetLearning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The optimiser can be chosen in the following way:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MLP with adam_lap optimizer\n",
    "mlp = MLPLearning(\n",
    "    train_optimizer={\n",
    "        'Adam_lap': {\n",
    "            'params': ['all'],\n",
    "            'lr': 0.01,\n",
    "            'lap_n': 20,\n",
    "            'depression_function': 'discrete_ranking_std',\n",
    "            'depression_function_kwargs': {},\n",
    "            'depression_strength': 1.0,\n",
    "            }\n",
    "        }\n",
    "    )\n",
    "\n",
    "# Conv3Net with sgd_lap\n",
    "cn = Conv3NetLearning(\n",
    "    train_optimizer={\n",
    "        'sgd_lap': {\n",
    "            'params': ['all'],\n",
    "            'lr': 0.01,\n",
    "            'momentum': 0.9,\n",
    "            'lap_n': 20,\n",
    "            'depression_function': 'discrete_ranking_std',\n",
    "            'depression_function_kwargs': {},\n",
    "            'depression_strength': 1.0,\n",
    "            }\n",
    "        }\n",
    "    )\n",
    "\n",
    "# ResNet with adam_lap\n",
    "rn = ResNetLearning(\n",
    "    train_optimizer={\n",
    "        'adam_lap': {\n",
    "            'params': ['all'],\n",
    "            'lr': 0.01,\n",
    "            'lap_n': 20,\n",
    "            'depression_function': 'discrete_ranking_std',\n",
    "            'depression_function_kwargs': {},\n",
    "            'depression_strength': 1.0,\n",
    "            }\n",
    "        }\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above, changing the ```depression_strength``` value from ```1.0``` to ```0.0``` will regain the standard model training and not apply LAP. Also, `sgd_lap` and `adam_lap` are already implemented through strings. If you want to use a different base optimiser, such as `Adagrad`, then you can specify it using: `'Adagrad_lap'`. An example is given for the MLP model above, with `'Adam_lap'`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or different models can be loaded using the ```synthetic_config.yaml``` file by specifying a model name.\n",
    "\n",
    "In the following example, we will load and fit the ```Conv3Net-c_lbf-drstd``` model. This is a convolutional network made for training on CIFAR-10 with label flipping corruption applied to the data. For an explanation of ```c_[CODE]``` codes, see the ```README.MD``` file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'model_name': 'Conv3Net-c_lbf-drstd',\n",
       " 'model_params': {'input_dim': 32,\n",
       "  'in_channels': 3,\n",
       "  'channels': 32,\n",
       "  'n_out': 10,\n",
       "  'n_epochs': 25,\n",
       "  'train_optimizer': {'adam_lap': {'params': ['all'],\n",
       "    'lr': 0.001,\n",
       "    'lap_n': 20,\n",
       "    'depression_strength': 1.0,\n",
       "    'depression_function': 'discrete_ranking_std',\n",
       "    'depression_function_kwargs': {'strictness': 0.8, 'hold_off': 0}}},\n",
       "  'train_criterion': 'CE'},\n",
       " 'train_params': {'train_method': 'traditional source',\n",
       "  'source_fit': True,\n",
       "  'n_sources': 10,\n",
       "  'source_size': 128,\n",
       "  'n_corrupt_sources': 4,\n",
       "  'corruption_function': 'label_flip',\n",
       "  'corruption_function_kwargs': {'source_save': True},\n",
       "  'validation': {'do_val': True, 'train_split': 0.75, 'corrupt': False}},\n",
       " 'save_params': {'model_path': './outputs/models/',\n",
       "  'result_path': './outputs/results/'}}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the following loads the model config for the model Conv3Net-c_lbf-drstd\n",
    "model_config = yaml.load(\n",
    "    open('./synthetic_config.yaml', 'r'),\n",
    "    Loader=yaml.FullLoader\n",
    "    )['Conv3Net-c_lbf-drstd']\n",
    "model_config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We make a model config for ```depression_strength``` equal to ```1.0``` and ```0.0```, so that we can compare them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LAP Model\n",
    "model_config_ds1 = copy.deepcopy(model_config)\n",
    "\n",
    "# Standard Model\n",
    "model_config_ds0 = copy.deepcopy(model_config)\n",
    "(model_config_ds0\n",
    "    ['model_params']\n",
    "    ['train_optimizer']\n",
    "    ['adam_lap']\n",
    "    ['depression_strength']\n",
    "    ) = 0.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This config can then be used to load the model class and to build the model with the ```get_model``` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from experiment_code.models.model_code.utils import get_model\n",
    "\n",
    "# LAP Model\n",
    "model_class_ds1 = get_model(model_config_ds1)\n",
    "model_ds1 = model_class_ds1(verbose=True, \n",
    "                            model_name=model_config_ds1['model_name'],\n",
    "                            **model_config_ds1['model_params'],\n",
    "                            **model_config_ds1['save_params']\n",
    "                            )\n",
    "\n",
    "# Standard Model\n",
    "model_class_ds0 = get_model(model_config_ds0)\n",
    "model_ds0 = model_class_ds0(verbose=True, \n",
    "                            model_name=model_config_ds0['model_name'],\n",
    "                            **model_config_ds0['model_params'],\n",
    "                            **model_config_ds0['save_params']\n",
    "                            )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given a training dataloader, this model can be easily fitted using the ```fit``` method. Training data can also be loaded using the ```model_config```, and ```args``` from ```argparse```. If using a notebook, you might want to use ```ArgFake``` to produce an ```args``` type object that can be used in replacement of ```argparse```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from experiment_code.utils.utils import ArgFake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = ArgFake({\n",
    "\n",
    "    'dataset_name': 'cifar10',\n",
    "    'verbose': True,\n",
    "    'data_dir': './data/',\n",
    "    'seed': 2,\n",
    "\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ```train_params``` part of the model config contains the information on how to corrupt the data, whilst ```args``` contains the dataset name and data directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'train_method': 'traditional source',\n",
       " 'source_fit': True,\n",
       " 'n_sources': 10,\n",
       " 'source_size': 128,\n",
       " 'n_corrupt_sources': 4,\n",
       " 'corruption_function': 'label_flip',\n",
       " 'corruption_function_kwargs': {'source_save': True},\n",
       " 'validation': {'do_val': True, 'train_split': 0.75, 'corrupt': False}}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_config_ds1['train_params']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data can be loaded with the ```get_train_data``` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      " --------- Producing the data loaders --------- \n"
     ]
    }
   ],
   "source": [
    "from experiment_code.data_utils.dataloader_loaders import get_train_data\n",
    "\n",
    "# data loaders are the same for model_config_ds1 and model_config_ds0\n",
    "train_loaders = get_train_data(args, model_config_ds1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This has produced a list of data loaders containing both the training and validation data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<torch.utils.data.dataloader.DataLoader at 0x26f1a3d63a0>,\n",
       " <torch.utils.data.dataloader.DataLoader at 0x26f1a3d64c0>]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_loaders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These can be used to fit the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training:    [##########]       Epoch: 25        Took: 1.7e+01s      Loss: 2.58e+00    Val Loss: 1.11e+00      Acc: 49.3%        Val Acc: 63.1%   Train Took: 4.4e+02s          \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Conv3NetLearning(\n",
       "  (net): Sequential(\n",
       "    (conv1): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=valid)\n",
       "    (relu1): ReLU()\n",
       "    (mp1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (conv2): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=valid)\n",
       "    (relu2): ReLU()\n",
       "    (mp2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (conv3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=valid)\n",
       "    (relu3): ReLU()\n",
       "    (flatten): Flatten(start_dim=1, end_dim=-1)\n",
       "  )\n",
       "  (pm_fc): Sequential(\n",
       "    (fc1): Linear(in_features=1024, out_features=64, bias=True)\n",
       "    (relu1): ReLU()\n",
       "  )\n",
       "  (pm_clf): Linear(in_features=64, out_features=10, bias=True)\n",
       "  (train_criterion): CrossEntropyLoss()\n",
       ")"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# LAP Model\n",
    "model_ds1.fit(train_loader=train_loaders[0], val_loader=train_loaders[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training:    [##########]       Epoch: 25        Took: 1.7e+01s      Loss: 1.82e+00    Val Loss: 1.56e+00      Acc: 40.6%        Val Acc: 49.5%   Train Took: 4.3e+02s          \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Conv3NetLearning(\n",
       "  (net): Sequential(\n",
       "    (conv1): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=valid)\n",
       "    (relu1): ReLU()\n",
       "    (mp1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (conv2): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=valid)\n",
       "    (relu2): ReLU()\n",
       "    (mp2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (conv3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=valid)\n",
       "    (relu3): ReLU()\n",
       "    (flatten): Flatten(start_dim=1, end_dim=-1)\n",
       "  )\n",
       "  (pm_fc): Sequential(\n",
       "    (fc1): Linear(in_features=1024, out_features=64, bias=True)\n",
       "    (relu1): ReLU()\n",
       "  )\n",
       "  (pm_clf): Linear(in_features=64, out_features=10, bias=True)\n",
       "  (train_criterion): CrossEntropyLoss()\n",
       ")"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Standard Model\n",
    "model_ds0.fit(train_loader=train_loaders[0], val_loader=train_loaders[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing the model's performance on the CIFAR-10 test data is similar. We use the test config, as well as a set of arguments to load the test data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from experiment_code.data_utils.dataloader_loaders import get_test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = ArgFake({\n",
    "\n",
    "    'dataset_name': 'cifar10',\n",
    "    'test_method': 'traditional',\n",
    "    'data_dir': './data/',\n",
    "    'verbose': True,\n",
    "    'seed': 2,\n",
    "\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a test config is loaded to ensure all testing is performed in the same way\n",
    "test_config = yaml.load(\n",
    "    open('./synthetic_config.yaml', 'r'),\n",
    "    Loader=yaml.FullLoader\n",
    "    )['testing-procedures'][args.test_method]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      " --------- Producing the data loaders --------- \n"
     ]
    }
   ],
   "source": [
    "# these configs are passed to the get_test_data function to load the data\n",
    "test_loader, test_targets = get_test_data(args, test_config=test_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The predictions can then be made by using the following method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting:  [####################]       Batch: 50          Took: 0.00     Predict Took: 1.8e+00s          \n"
     ]
    }
   ],
   "source": [
    "outputs_ds1 = model_ds1.predict(test_loader=test_loader, targets_too=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy with LAP was 63.20%\n"
     ]
    }
   ],
   "source": [
    "confidence, predictions = outputs_ds1.max(dim=1)\n",
    "accuracy = torch.sum(predictions == test_targets)/len(test_targets)\n",
    "print('Accuracy with LAP was {:.2f}%'.format(accuracy*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting:  [####################]       Batch: 50          Took: 0.00     Predict Took: 1.8e+00s          \n"
     ]
    }
   ],
   "source": [
    "outputs_ds0 = model_ds0.predict(test_loader=test_loader, targets_too=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy with the standard model was 50.03%\n"
     ]
    }
   ],
   "source": [
    "confidence, predictions = outputs_ds0.max(dim=1)\n",
    "accuracy = torch.sum(predictions == test_targets)/len(test_targets)\n",
    "print('Accuracy with the standard model was {:.2f}%'.format(accuracy*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Graphs:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running the code specified in the ```README.MD``` will save models in the `outputs/models/` directory, training loss graphs in the `outputs/results/` directory, test results in the `outputs/synthetic_results/` directory, and graphs in the `outputs/graphs/`.\n",
    "\n",
    "The below is an exmaple of the graphs produced:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"./outputs/graphs/accuracy_results_complete.png\" width=\"600\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import Image\n",
    "Image(url=\"./outputs/graphs/accuracy_results_complete.png\", width=600,)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After running the code in the `graphs_[EXPERIMENT NAME].ipynb` notebooks, the `outputs/graphs/` directory will update with the graphs"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d967f5200d3e7bff89c1fd5f531c8f128a67df959ac904883da659ba40a51b47"
  },
  "kernelspec": {
   "display_name": "Python 3.8.13 ('dri')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
